

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
from torch.autograd import Variable

# Training settings
batch_size = 64

# MNIST Dataset
train_dataset = datasets.MNIST(root='./mnist_data/',
                               train=True,
                               transform=transforms.ToTensor(),
                               download=True)

test_dataset = datasets.MNIST(root='./mnist_data/',
                              train=False,
                              transform=transforms.ToTensor())

# Data Loader (Input Pipeline)
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.L1 = nn.Linear(784, 520)
        self.L2 = nn.Linear(520, 320)
        self.L3 = nn.Linear(320, 240)
        self.L4 = nn.Linear(240, 120)
        self.L5 = nn.Linear(120, 10)

    def forward(self, x):
        # Flatten the data (n, 1, 28, 28) --> (n, 784)
        x = x.view(-1, 784)
        x = F.relu(self.L1(x))
        x = F.relu(self.L2(x))
        x = F.relu(self.L3(x))
        x = F.relu(self.L4(x))
        return F.log_softmax(self.L5(x), dim=1)
        #return self.l5(x)
```


```python
model = Net()

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)

def train(epoch):
    # 每次输入barch_idx个数据
 
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = Variable(data), Variable(target)

        optimizer.zero_grad()
        output = model(data)
                
        
        # loss
        loss = F.nll_loss(output, target)
        loss.backward()
        # update
        optimizer.step()
        if batch_idx % 200 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))
        

        


def test():
    test_loss = 0
    correct = 0
    # 测试集
    for data, target in test_loader:
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data)
        # sum up batch loss
        test_loss += F.nll_loss(output, target).data[0]
        # get the index of the max
        pred = output.data.max(1, keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).cpu().sum()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))


```


```python
     
for epoch in range(1,6):
    train(epoch)
test()
```

    d:\Users\Administrator\Anaconda3\lib\site-packages\ipykernel_launcher.py:23: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
    

    Train Epoch: 1 [0/60000 (0%)]	Loss: 2.306398
    Train Epoch: 1 [12800/60000 (21%)]	Loss: 2.287521
    Train Epoch: 1 [25600/60000 (43%)]	Loss: 2.268071
    Train Epoch: 1 [38400/60000 (64%)]	Loss: 2.242717
    Train Epoch: 1 [51200/60000 (85%)]	Loss: 1.994630
    Train Epoch: 2 [0/60000 (0%)]	Loss: 1.339015
    Train Epoch: 2 [12800/60000 (21%)]	Loss: 0.793334
    Train Epoch: 2 [25600/60000 (43%)]	Loss: 0.570069
    Train Epoch: 2 [38400/60000 (64%)]	Loss: 0.597942
    Train Epoch: 2 [51200/60000 (85%)]	Loss: 0.349046
    Train Epoch: 3 [0/60000 (0%)]	Loss: 0.513144
    Train Epoch: 3 [12800/60000 (21%)]	Loss: 0.336468
    Train Epoch: 3 [25600/60000 (43%)]	Loss: 0.529630
    Train Epoch: 3 [38400/60000 (64%)]	Loss: 0.212095
    Train Epoch: 3 [51200/60000 (85%)]	Loss: 0.392071
    Train Epoch: 4 [0/60000 (0%)]	Loss: 0.281587
    Train Epoch: 4 [12800/60000 (21%)]	Loss: 0.230086
    Train Epoch: 4 [25600/60000 (43%)]	Loss: 0.280261
    Train Epoch: 4 [38400/60000 (64%)]	Loss: 0.211534
    Train Epoch: 4 [51200/60000 (85%)]	Loss: 0.203806
    Train Epoch: 5 [0/60000 (0%)]	Loss: 0.211825
    Train Epoch: 5 [12800/60000 (21%)]	Loss: 0.244602
    Train Epoch: 5 [25600/60000 (43%)]	Loss: 0.225040
    Train Epoch: 5 [38400/60000 (64%)]	Loss: 0.113788
    Train Epoch: 5 [51200/60000 (85%)]	Loss: 0.100808
    

    d:\Users\Administrator\Anaconda3\lib\site-packages\ipykernel_launcher.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.
    d:\Users\Administrator\Anaconda3\lib\site-packages\ipykernel_launcher.py:37: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number
    

    
    Test set: Average loss: 0.0026, Accuracy: 9512/10000 (95%)
    
    


```python
param = list(model.parameters())
print(param)
```

    [Parameter containing:
    tensor([[ 1.2976e-02, -1.2288e-03, -1.9785e-03,  ...,  2.0297e-03,
              3.3121e-02,  2.6630e-02],
            [ 3.3138e-02, -4.3824e-03, -7.0144e-03,  ...,  4.2771e-03,
             -8.3251e-03, -7.5786e-03],
            [-1.7827e-03, -2.9558e-02, -8.0329e-03,  ...,  8.1846e-03,
             -1.8375e-02,  1.2090e-02],
            ...,
            [ 1.9376e-02,  1.3008e-03, -9.1713e-04,  ...,  3.4041e-02,
              2.5594e-02,  2.4830e-02],
            [-2.6149e-02,  3.0978e-02, -9.5143e-03,  ...,  1.2855e-02,
              7.0281e-03, -1.2823e-02],
            [ 2.0202e-02, -1.6103e-02,  3.3596e-02,  ...,  2.6706e-02,
              2.1830e-02,  2.6355e-02]]), Parameter containing:
    tensor(1.00000e-02 *
           [ 4.1891, -0.0596,  5.2874,  1.3783, -1.0213, -0.5160,  3.2175,
            -2.8359, -2.8782, -2.3199,  1.8364, -1.2491,  3.5297,  1.9640,
             0.0741,  3.2897,  2.6678,  0.5193,  2.9370,  0.3476, -1.2703,
             2.8276,  2.7434,  4.7723, -2.3559,  1.0153,  2.6548,  1.3039,
             1.5479, -1.7794,  1.9216, -2.1400,  1.7944,  1.8903,  2.3856,
             3.2758,  4.4861, -0.8891, -1.0883, -0.7650,  0.4914, -2.3560,
            -3.4385, -0.0434,  3.1769, -0.8586,  4.3512, -1.7727, -1.5506,
             3.2597,  3.8717,  2.2592,  0.2341, -3.0467,  2.6914,  0.3595,
            -0.2067, -1.7551,  1.0835,  4.2787,  0.0998,  1.3569, -1.7441,
             1.2297, -0.9131, -2.2740, -0.2644,  2.3528, -0.7324,  0.1415,
             4.0191, -0.0309,  0.1935, -3.7381, -1.5221, -1.1408,  0.7766,
            -0.2052,  2.1204, -0.9020, -1.1936,  2.9920, -1.6499, -0.3698,
             3.8959, -1.5637,  3.3818,  3.8554,  0.1073, -1.0578,  2.2942,
             1.1534,  2.7285, -0.4215,  3.0409, -1.0650, -2.4625, -0.1399,
            -1.9528, -0.5058,  4.3266, -2.9187, -0.9969,  0.9053,  0.7890,
             1.5407,  2.0655,  0.5474, -0.9733, -3.2618,  2.1066,  0.1070,
            -2.8433,  3.6701,  0.7549, -2.7867, -4.3815, -2.7746, -2.6244,
            -3.9069,  3.9054,  1.6818, -1.6726,  4.4427, -0.4349, -3.7482,
             1.8838, -2.2423,  0.1389,  3.6280, -1.4960, -2.5163,  1.6844,
             2.0728, -0.9133, -1.0215,  2.9573,  6.0751,  3.9528, -1.8958,
             2.5900, -2.2668,  0.6304,  0.2432, -2.2184, -2.6960, -0.4352,
             3.7367, -1.7341,  0.7370, -1.4625,  4.2903,  3.6312, -2.8433,
             0.0805,  0.2256, -2.5014,  0.9582, -2.3245,  2.1359,  2.4304,
             3.1417,  2.7130, -1.0155, -0.0844, -2.9121, -1.6332,  1.9197,
             1.7225,  3.1030, -0.7839, -1.2982,  0.9737,  1.6377, -2.1023,
            -1.1218, -2.5832,  2.9686, -1.4155, -1.9024,  0.7534, -0.9381,
            -0.7065, -2.4449,  5.9934,  0.2246,  2.8499,  2.4008,  2.5981,
             2.7283,  3.4516, -0.7273, -2.1485,  2.1263, -3.6495,  0.9428,
            -2.2263,  3.9488, -3.4703, -0.6867,  1.7217, -0.7081,  0.6797,
             3.5211, -0.0515,  0.3578,  1.6793, -1.9596, -0.7947, -2.8451,
             0.3640,  0.6645, -1.0298,  0.0122, -2.7030, -2.9121,  1.1426,
            -4.2570,  3.1054,  3.3740,  1.0447,  1.3114,  0.4855,  0.6347,
            -1.0692,  4.2768, -2.6689,  1.8330,  0.2658,  2.3857, -0.8255,
             1.4638, -1.8270, -1.1206, -2.6207,  4.6435,  4.3625,  2.6291,
             1.8462,  1.4377, -1.5336,  1.3257,  1.4241,  1.4932,  1.2026,
             1.2935, -2.1708,  0.0938,  4.1599,  1.2786,  0.7166,  3.5212,
             3.6950,  1.3024, -1.6916, -1.2144,  4.5618,  0.4652, -0.4951,
             2.3156, -2.1093, -1.1725,  2.2881,  0.8786, -0.9598,  4.0367,
             2.7310,  2.2036, -1.8365,  1.5451, -0.9232,  1.5025,  0.7369,
             2.1975,  1.1942, -1.9642,  0.1812,  1.0015,  3.5108,  1.9809,
             2.3278, -1.2713,  4.7716, -1.1520,  0.4305,  3.6594, -1.2416,
            -0.4080,  3.6470, -4.5186, -1.6667,  1.2995, -0.7466, -1.8672,
             2.5130, -2.5726,  1.8333, -0.0332,  2.4758,  0.0237,  1.1799,
             4.1521,  0.0969,  3.8723, -2.1397,  0.6322, -3.2252,  4.1348,
            -1.8742, -1.1432,  2.5712, -0.3443,  1.6023, -1.5686, -2.1531,
             2.4557,  6.1584, -0.7485, -0.9688,  2.8805,  0.6377,  1.3932,
            -0.6465,  0.1464,  1.1991,  1.4773,  2.8150,  0.1334, -1.4927,
             1.8410, -0.7694,  1.8665,  2.7766,  2.5090, -0.8755,  1.9385,
            -0.4572,  1.1445, -2.4820, -3.0739, -2.2838, -2.0748, -3.3616,
            -0.0133, -0.9444,  4.5391,  2.2518, -2.3830, -0.0265, -0.3947,
             4.0375,  0.9382,  3.5546, -2.5738,  2.5073,  0.2588,  2.1439,
             0.1609, -0.0452,  2.3547, -0.2031,  2.4324, -1.0840, -3.3113,
            -2.7406,  1.2577, -2.5103, -2.4319,  2.5224, -1.0903, -2.0392,
             0.2967,  2.7597, -0.2953,  1.8892, -1.5787, -2.4632, -1.8929,
            -2.4875,  1.4708,  0.1334, -1.8897,  1.9470, -2.9944, -2.0548,
             2.2195,  1.8276,  1.0074, -1.1549,  1.6555, -1.7388, -0.9914,
             3.4707,  2.0999,  0.5518,  2.8552, -1.2971,  2.9409,  4.2185,
             4.8528,  2.8228, -0.7309,  0.3134,  5.4474,  3.9640, -0.2237,
             2.7054, -3.1018, -1.2372,  3.0641,  2.1538,  3.2815,  4.5360,
            -1.5586,  2.9369, -1.2883, -1.4240,  3.3963, -1.8096, -0.6032,
            -0.9895, -1.8862,  1.7835, -2.5637,  1.7546, -0.2058, -2.7210,
             4.4204,  0.2078,  3.7032, -3.1772,  1.8881, -2.6078, -1.8796,
            -2.2060, -0.9164, -2.2621,  2.5060, -2.2341, -1.0917,  3.3996,
             3.4965, -2.6862,  4.6774,  2.2713, -2.2290, -0.3923,  2.8741,
            -2.6640,  2.6591,  4.6608,  0.4947,  1.4581,  2.1744,  0.9115,
             1.5250, -2.9079,  2.5813,  0.6879,  1.9459, -0.6238,  3.4524,
            -0.5061,  2.3724,  3.0734,  0.0455,  1.1536, -0.7900,  4.4785,
             0.0142,  0.0266,  0.4355,  0.3514,  5.0097, -0.4555, -1.3706,
            -0.0658,  0.0963, -0.9542,  0.6605, -1.2635, -2.0722,  5.1769,
            -2.0553,  2.5453, -1.5212,  2.6948,  2.0625,  1.8223, -2.6008,
            -1.8355, -2.8689,  2.4608, -2.8664,  1.6336,  4.7842,  0.0545,
             2.0705,  2.8354,  2.8614, -1.4366, -1.9472,  2.2708,  3.7474,
            -0.1191,  0.2788,  3.2687, -0.3466,  3.4417,  1.9033,  0.3620,
             2.5679, -2.3209,  0.3770, -1.8966, -3.1388, -3.0509,  0.8032,
             2.0857, -2.9051]), Parameter containing:
    tensor([[-4.7079e-02, -2.1902e-02,  1.7259e-02,  ..., -3.0975e-02,
             -4.0276e-02,  4.1265e-02],
            [ 4.1291e-02, -4.4927e-03,  2.6979e-03,  ...,  4.7694e-02,
             -1.7597e-02, -1.6576e-02],
            [ 3.0611e-02,  2.0509e-02,  4.2167e-02,  ...,  4.0957e-02,
             -3.6699e-02, -2.8122e-02],
            ...,
            [-8.9454e-03,  2.7488e-03,  2.5761e-02,  ...,  3.8021e-02,
             -9.6328e-03, -3.0995e-02],
            [-1.1449e-02, -2.7587e-03,  4.3134e-02,  ..., -2.5869e-02,
              3.6900e-02,  1.1397e-02],
            [-3.8288e-02, -1.3896e-02,  9.2364e-03,  ..., -2.0917e-02,
              2.6317e-03, -2.7777e-02]]), Parameter containing:
    tensor(1.00000e-02 *
           [ 2.7549,  4.0481, -2.1803,  3.8134, -4.4594,  5.2427,  1.1377,
            -0.5609, -0.3153, -3.3561,  1.6808, -4.0531,  4.5399,  3.0287,
            -1.2885,  3.2444, -2.8417, -1.6990,  3.0917, -2.2770, -2.8271,
             5.4766, -0.3433,  2.0143,  3.1306,  3.6120,  3.7406,  2.4247,
             0.3068,  2.4088,  2.7748,  2.9567,  4.7948,  2.5355,  5.5388,
            -4.1795,  3.0471,  2.6523,  0.2460,  4.2524,  0.3691,  6.9855,
            -3.0969, -4.4465,  5.7517,  3.7426, -2.5414,  1.0972, -1.0101,
             0.8708, -0.3768,  2.9309,  3.6986,  2.4156, -2.2855,  4.0000,
            -3.3073, -0.9083,  5.8546,  4.5954,  1.2268, -4.5999,  0.3926,
             1.4999,  1.1981,  3.1697,  3.9379,  6.0095,  1.7970,  6.0299,
             7.7094,  0.7797, -1.7010,  4.6889, -2.8759, -0.5165, -0.5221,
            -3.6928, -3.4221,  0.1815,  2.1523,  2.3357,  6.4580,  3.7135,
            -0.4387,  2.9187,  1.1511,  0.2425,  0.0416,  3.3922,  2.0563,
            -4.2698, -1.0586, -1.5149, -3.0967,  1.6320, -1.5372, -3.3636,
            -0.4757, -4.3597, -1.5337,  4.1291,  3.8865, -0.7813,  2.0433,
             1.0934, -3.5338,  2.9956,  0.4996,  4.2020, -1.1327,  0.6730,
            -0.3543,  0.1360,  2.7953,  3.6853,  2.0839,  2.7749,  1.6794,
             2.1965,  0.8978,  1.6422,  2.8433, -1.7237,  4.7248,  7.7634,
             3.5483,  2.5426,  2.7295, -2.1856,  3.2717,  2.5978,  1.2109,
             0.7890, -0.5598,  2.4642,  1.2792,  3.3042, -1.3568, -1.1664,
             3.1617, -0.9246, -3.5935,  3.3884,  5.1030, -2.2599,  0.2311,
            -3.6403,  1.2564, -0.9088,  3.5592,  5.0075,  2.8053,  0.2324,
            -4.8347, -0.8552, -2.3222, -1.9618, -0.3996, -3.9425,  5.4570,
             3.3053, -1.2520, -4.7949,  3.6611,  0.3126,  4.6648,  0.6965,
            -2.9192,  4.9584, -2.1149, -1.2786, -0.1455,  1.5121,  1.8763,
            -1.9668,  1.7313,  0.7496,  1.1719,  5.4524,  6.5096, -1.0957,
             1.5489, -0.1797,  3.9713, -2.1699, -1.9108, -4.8575, -2.4127,
             5.2126,  2.6198, -2.8012,  1.6975, -2.0359,  5.1790, -0.6077,
             4.6982,  2.7480,  4.6209,  0.7553,  4.2244, -2.0463,  3.7816,
            -0.5114, -0.8942,  3.0274, -3.8753, -2.3656,  4.6401,  2.1965,
            -1.0297,  4.9776, -0.7331, -0.6670, -4.4278,  6.8165, -0.0618,
            -4.0113,  0.0796,  0.2224,  3.4407, -1.2924,  5.3236,  0.5754,
             1.1837,  1.4274, -1.9611,  2.8833, -2.2219, -2.0353,  1.4414,
            -1.6000, -3.7723, -3.3777, -4.7876,  4.4764,  2.1051,  2.6926,
            -1.2424, -3.8001,  1.4705,  3.0122,  3.7282,  0.8586,  3.8575,
             3.5076,  4.8728,  1.4026,  6.1508, -0.8183,  1.8982, -3.6657,
             3.2912,  0.9513,  0.1809,  4.4758,  1.2858, -3.9610,  5.6602,
            -1.2214, -0.6316,  6.8959,  7.5187, -1.8628, -3.6611,  0.3445,
             4.2922,  3.6568,  2.2578,  3.6976, -2.2042,  3.3878, -1.4124,
             5.5873,  1.4891,  1.5901,  3.6002,  5.1001, -1.3135,  3.4735,
             3.1714, -1.5822,  3.5693,  2.7636,  7.4750, -2.3958,  0.4126,
            -3.2794,  3.5599,  4.9121,  4.2993,  0.7642,  3.4552, -1.6476,
             2.6559,  5.3530, -4.1904, -1.8234, -1.9178, -2.3402,  5.2108,
             4.2355,  4.2780,  4.0933,  0.7789, -0.7714,  0.4625, -3.4655,
            -0.8386, -0.0073, -3.2017,  2.0695,  1.5106,  3.2295, -3.6601,
            -0.3052,  3.5834,  3.6871,  3.7965,  0.5406]), Parameter containing:
    tensor([[-2.9466e-02,  4.7197e-03,  2.1299e-02,  ...,  8.7857e-02,
              5.5261e-02, -4.3748e-04],
            [-3.9892e-02, -5.0865e-02,  3.7316e-03,  ..., -4.1963e-02,
              1.9062e-02,  5.2947e-02],
            [ 2.3762e-02, -2.1221e-02, -2.1344e-02,  ...,  5.4931e-02,
              3.7350e-02, -2.7383e-02],
            ...,
            [-5.4485e-02,  4.3698e-02, -3.0817e-02,  ...,  5.5657e-03,
              4.8613e-04,  9.6668e-03],
            [ 4.5732e-02,  2.9586e-02, -3.2194e-02,  ..., -1.9837e-02,
              1.6823e-02,  2.5058e-02],
            [ 1.8069e-02,  1.8862e-02, -3.1320e-02,  ..., -1.3804e-02,
              3.6321e-02,  4.8942e-03]]), Parameter containing:
    tensor([ 0.0184,  0.0560, -0.0162, -0.0171,  0.0456, -0.0068, -0.0269,
             0.0522,  0.0811,  0.0613,  0.0240, -0.0465,  0.0163,  0.0538,
             0.0549,  0.0128,  0.0177,  0.0359,  0.0441, -0.0317,  0.0689,
             0.0469,  0.0403,  0.0481, -0.0102,  0.0641,  0.0111, -0.0142,
             0.0096, -0.0478,  0.0470, -0.0302,  0.0751,  0.0431, -0.0167,
            -0.0014, -0.0146,  0.0972, -0.0149,  0.0170, -0.0517,  0.0130,
            -0.0452,  0.0416,  0.0082,  0.0711,  0.0645,  0.0460, -0.0121,
             0.0613, -0.0066, -0.0248, -0.0306,  0.0355,  0.0064, -0.0500,
             0.0006,  0.0361,  0.0073, -0.0298, -0.0092,  0.0215,  0.0187,
             0.0588, -0.0277, -0.0286, -0.0359,  0.0582, -0.0405,  0.0148,
             0.0252,  0.0887, -0.0076, -0.0247,  0.0548, -0.0242,  0.0579,
            -0.0167, -0.0623, -0.0385,  0.0057,  0.0942, -0.0725,  0.0797,
             0.0647,  0.0057, -0.0410,  0.0606, -0.0671,  0.0239,  0.0695,
             0.0472,  0.0089, -0.0453, -0.0052, -0.0046,  0.0168, -0.0127,
            -0.0225,  0.0830,  0.0895,  0.0568, -0.0144,  0.0087, -0.0146,
            -0.0488,  0.1119,  0.0251,  0.0016, -0.0366,  0.0001, -0.0618,
             0.0569,  0.0514,  0.1017,  0.0448,  0.0165,  0.0169,  0.0562,
            -0.0233,  0.0058,  0.0142, -0.0330,  0.0621, -0.0401, -0.0372,
            -0.0243, -0.0084,  0.0603, -0.0067,  0.0547,  0.0496,  0.0198,
            -0.0473,  0.0318,  0.1102, -0.0387, -0.0270, -0.0217, -0.0372,
             0.0380,  0.0163,  0.0983,  0.0590, -0.0358,  0.0516, -0.0269,
             0.0616,  0.0260,  0.0079, -0.0249,  0.0457,  0.0715,  0.0739,
             0.0715,  0.0334, -0.0213,  0.0697, -0.0559, -0.0340, -0.0475,
             0.0281,  0.0234, -0.0257,  0.0679,  0.0061,  0.0366,  0.0793,
             0.0252, -0.0224,  0.0264,  0.0806, -0.0056, -0.0104,  0.0004,
             0.0837,  0.0696, -0.0283,  0.0182, -0.0344,  0.0071, -0.0487,
             0.0161, -0.0110, -0.0105, -0.0476, -0.0582,  0.0855, -0.0400,
             0.0648,  0.0560,  0.0916, -0.0209,  0.0296, -0.0279,  0.0589,
            -0.0496, -0.0065,  0.0145,  0.0025,  0.0682, -0.0368,  0.0737,
             0.0776,  0.0974, -0.0348,  0.0228, -0.0633, -0.0549, -0.0222,
             0.0292,  0.0128, -0.0443,  0.0520, -0.0381,  0.0422, -0.0452,
             0.0803, -0.0135, -0.0129,  0.0289,  0.0341,  0.0872,  0.0206,
            -0.0261, -0.0253,  0.0679, -0.0006, -0.0214, -0.0135, -0.0343,
             0.0079,  0.0935,  0.0105, -0.0077, -0.0175,  0.0650, -0.0320,
             0.0559,  0.0330]), Parameter containing:
    tensor([[ 3.7533e-02,  8.2425e-02,  6.2669e-02,  ...,  6.5245e-03,
             -1.3861e-02,  5.8398e-02],
            [-3.8007e-02,  1.3945e-02, -2.1812e-02,  ...,  3.2896e-02,
              5.1262e-03,  4.1999e-02],
            [ 5.2205e-02,  1.9902e-03,  2.1312e-02,  ..., -3.6500e-02,
              3.7773e-02, -4.7541e-02],
            ...,
            [-6.3127e-03, -3.1373e-02, -5.5311e-02,  ...,  5.6046e-02,
              7.3091e-02, -6.6140e-03],
            [ 8.6008e-02, -1.5405e-02,  4.1472e-02,  ...,  1.3651e-02,
             -1.7514e-02,  1.9101e-02],
            [ 1.8724e-02, -7.7540e-02, -3.5825e-02,  ...,  2.0269e-02,
             -7.3672e-02,  5.4049e-02]]), Parameter containing:
    tensor([ 0.0161, -0.0337, -0.0386,  0.0057,  0.0503, -0.0271,  0.0630,
             0.1502,  0.1365,  0.0489,  0.0441, -0.0614,  0.0019, -0.0124,
             0.0506,  0.0161, -0.0443, -0.0378, -0.0023,  0.0793, -0.0732,
             0.0044,  0.0564, -0.0499, -0.0119,  0.0259, -0.0276, -0.0034,
             0.0706,  0.0375,  0.0453, -0.0274, -0.0444, -0.0502,  0.0904,
             0.0497, -0.0155,  0.0476,  0.0191, -0.0347, -0.0447,  0.1216,
             0.1480,  0.0873,  0.1258,  0.0580, -0.0484, -0.0202,  0.1201,
             0.0566, -0.0447,  0.0276,  0.0118,  0.0518, -0.1060,  0.0100,
             0.0137,  0.0033, -0.0602, -0.1079,  0.0789,  0.0056, -0.0103,
             0.0845, -0.0149, -0.0550, -0.0158,  0.1313,  0.1620,  0.0946,
             0.0561, -0.0069,  0.0387,  0.0575, -0.0495,  0.0447, -0.0746,
             0.0109, -0.0229, -0.0061,  0.1254, -0.0008,  0.0882,  0.0544,
             0.0857,  0.0697,  0.0911, -0.0576, -0.0272,  0.0832, -0.0134,
            -0.0552,  0.0222, -0.0154,  0.1718, -0.0616, -0.0885, -0.0220,
             0.0970,  0.0891, -0.0638,  0.0607,  0.0767,  0.1206, -0.0140,
             0.0881, -0.0548, -0.0590,  0.0632,  0.0297,  0.0127,  0.0703,
            -0.0214,  0.0528, -0.0293,  0.0782,  0.0312, -0.0544,  0.0038,
             0.0639]), Parameter containing:
    tensor([[-0.0547, -0.0760,  0.0827,  ...,  0.0137,  0.0702,  0.0986],
            [ 0.0336, -0.0464, -0.0666,  ...,  0.0342, -0.1710, -0.2587],
            [ 0.1406,  0.1204,  0.0601,  ...,  0.0003, -0.1428, -0.0151],
            ...,
            [-0.0685,  0.0322, -0.0119,  ..., -0.0113,  0.0905,  0.0871],
            [-0.1055, -0.0640,  0.0879,  ...,  0.1508,  0.1215, -0.0751],
            [-0.1236,  0.0012, -0.0371,  ..., -0.0037,  0.1818,  0.1465]]), Parameter containing:
    tensor([-0.1922,  0.3284,  0.0629, -0.0278,  0.0698,  0.0137,  0.0166,
             0.0779, -0.3699,  0.0162])]
    


```python
print(model)
```

    Net(
      (L1): Linear(in_features=784, out_features=520, bias=True)
      (L2): Linear(in_features=520, out_features=320, bias=True)
      (L3): Linear(in_features=320, out_features=240, bias=True)
      (L4): Linear(in_features=240, out_features=120, bias=True)
      (L5): Linear(in_features=120, out_features=10, bias=True)
    )
    


```python
import torchvision
import matplotlib.pyplot as plt

X_test,y_test = next(iter(test_loader))
y_test=y_test.view(8,8)
print("Test dataset label is:\n{}".format(y_test))
X = torchvision.utils.make_grid(X_test)
X = X.numpy().transpose(1,2,0)
plt.imshow(X)
```

    Test dataset label is:
    tensor([[ 7,  2,  1,  0,  4,  1,  4,  9],
            [ 5,  9,  0,  6,  9,  0,  1,  5],
            [ 9,  7,  3,  4,  9,  6,  6,  5],
            [ 4,  0,  7,  4,  0,  1,  3,  1],
            [ 3,  4,  7,  2,  7,  1,  2,  1],
            [ 1,  7,  4,  2,  3,  5,  1,  2],
            [ 4,  4,  6,  3,  5,  5,  6,  0],
            [ 4,  1,  9,  5,  7,  8,  9,  3]])
    




    <matplotlib.image.AxesImage at 0x2212540e6a0>




```python
print (X.size)
```

    175692
    


```python
plt.show()
```


![png](output_7_0.png)



```python
y=model(X_test)
pred = y.data.max(1, keepdim=True)[1]
```


```python
pred=pred.view(8,8)
print(pred)
```

    tensor([[ 7,  2,  1,  0,  4,  1,  4,  9],
            [ 6,  9,  0,  6,  9,  0,  1,  5],
            [ 9,  7,  3,  4,  9,  6,  6,  5],
            [ 4,  0,  7,  4,  0,  1,  3,  1],
            [ 3,  6,  7,  2,  7,  1,  2,  1],
            [ 1,  7,  4,  2,  3,  5,  1,  2],
            [ 4,  4,  6,  3,  5,  5,  6,  0],
            [ 4,  1,  9,  5,  7,  8,  9,  3]])
    

网络权重初始化 采用全1初始化网络，网络测试准确率非常低11%
可考虑其它方法初始化网络，learnning...


```python
def init_weights(m):
        print(m)
        if type(m) == nn.Linear:
            m.weight.data.fill_=(1.0)
            print(m.weight)

net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
net.apply(init_weights)
```

    Linear(in_features=2, out_features=2, bias=True)
    Parameter containing:
    tensor([[-0.3755, -0.1032],
            [ 0.6824,  0.6253]])
    Linear(in_features=2, out_features=2, bias=True)
    Parameter containing:
    tensor([[-0.3748, -0.3148],
            [ 0.2104,  0.3318]])
    Sequential(
      (0): Linear(in_features=2, out_features=2, bias=True)
      (1): Linear(in_features=2, out_features=2, bias=True)
    )
    




    Sequential(
      (0): Linear(in_features=2, out_features=2, bias=True)
      (1): Linear(in_features=2, out_features=2, bias=True)
    )




```python

param = list(net.parameters())
```


```python
print(net)
```

    Sequential(
      (0): Linear(in_features=2, out_features=2, bias=True)
      (1): Linear(in_features=2, out_features=2, bias=True)
    )
    


```python
print(param)
```

    [Parameter containing:
    tensor([[-0.3755, -0.1032],
            [ 0.6824,  0.6253]]), Parameter containing:
    tensor([ 0.2743, -0.1800]), Parameter containing:
    tensor([[-0.3748, -0.3148],
            [ 0.2104,  0.3318]]), Parameter containing:
    tensor([-0.4141,  0.0516])]
    
